{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from googletrans import Translator\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from fancyimpute import KNN, IterativeImputer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data Review\n",
    "\n",
    "Given that the dataset has now been cleaned, we can move onto reviewing missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12    37855\n",
      "13    22357\n",
      "10    16636\n",
      "11    13939\n",
      "9      4652\n",
      "7      2220\n",
      "8      1852\n",
      "14      849\n",
      "6       456\n",
      "4       177\n",
      "5       122\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_pickle('cleaned_df.pkl')\n",
    "\n",
    "# Review missing across rows\n",
    "print(df.count(axis=1).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can drop rows with a certain level missing. Given that there are 14 columns, removing those with less than 10 completed would still keep a large portion of the data but give us a clearer picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other_cat1      0.019239\n",
      "other_cat2      0.029672\n",
      "other_cat3      0.002739\n",
      "other_cat4      0.002739\n",
      "ordinal1        0.231699\n",
      "type            0.372223\n",
      "location3       0.000000\n",
      "location4       0.000000\n",
      "location5       0.011633\n",
      "ordinal2        0.558623\n",
      "other_cat5      0.270014\n",
      "sector2         0.754114\n",
      "label           0.000000\n",
      "adj_m_signup    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Given 14 columns, drop rows with 10 or less features\n",
    "df.dropna(thresh=10,axis=0, inplace=True)\n",
    "\n",
    "# Review missing data - columms\n",
    "print(df.isna().sum()/len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review those missing variables more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other_cat1\n",
      "Most Frequent: Signup, Percentage: 0.9808\n",
      "Missing 0.0192\n",
      "\n",
      "other_cat2\n",
      "Most Frequent: Signup, Percentage: 0.9430\n",
      "Missing 0.0297\n",
      "\n",
      "other_cat3\n",
      "Most Frequent: Google, Percentage: 0.5067\n",
      "Missing 0.0027\n",
      "\n",
      "other_cat4\n",
      "Most Frequent: Home, Percentage: 0.3274\n",
      "Missing 0.0027\n",
      "\n",
      "ordinal1\n",
      "Most Frequent: 1.0, Percentage: 0.9117\n",
      "Missing 0.2317\n",
      "\n",
      "type\n",
      "Most Frequent: isp, Percentage: 0.9681\n",
      "Missing 0.3722\n",
      "\n",
      "location5\n",
      "Most Frequent: 2.0, Percentage: 0.1854\n",
      "Missing 0.0116\n",
      "\n",
      "ordinal2\n",
      "Most Frequent: 5.0, Percentage: 0.3839\n",
      "Missing 0.5586\n",
      "\n",
      "other_cat5\n",
      "Most Frequent: developer, Percentage: 0.7529\n",
      "Missing 0.2700\n",
      "\n",
      "sector2\n",
      "Most Frequent: Information Technology, Percentage: 0.5453\n",
      "Missing 0.7541\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Review Conversion Rate Between Missing Vars\n",
    "missing_cols = df.columns[df.isnull().any()]\n",
    "\n",
    "# Find Percentages of Most Frequent\n",
    "def freq_perc(col):\n",
    "    x = df[col].dropna()\n",
    "    max_val = x.value_counts().idxmax()\n",
    "    max_perc = x.value_counts().max() / len(x)\n",
    "    missing_perc = 1-(len(x.dropna())/len(df))\n",
    "    print('{}\\nMost Frequent: {}, Percentage: {:.4f}'.format(col, max_val, max_perc))\n",
    "    print('Missing {:.4f}\\n'.format(missing_perc))\n",
    "\n",
    "[freq_perc(x) for x in missing_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those categorical variables that are missing, the most frequent inputs either have a very large proportion, or the missingness of the variable itself is very low. However, sector is both highly missing and it's most frequent input only accounts for about half of the input set. We can impute the most frequent for all categorical variables outisde of category sector for the time being. \n",
    "\n",
    "For those continuous variables that are missing, ordinal1 is highly skewed so most frequent imputation would make sense. For ordinal2, we can impute the median value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sector2'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Use most frequent for section of columns\n",
    "freq_cols = [x for x in missing_cols if x not in ['ordinal2','sector2']]\n",
    "\n",
    "for col in freq_cols:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "##### Use median for employee size (continuous var); \n",
    "df['ordinal2'].fillna(df.ordinal2.dropna().median(), inplace=True)\n",
    "\n",
    "# Review missing\n",
    "df.columns[df.isnull().any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For imputing sectors, we can use KNN imputation to find those combination of other columns that are most similar and impute the value found therein for category. To do this, I will use an encoding process that allows categorical features to be encoded for the imputation process, and then inverse transformed to keep the original string values for further modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiColumnLabelEncoder:\n",
    "\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns \n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoders = {}\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            self.encoders[col] = LabelEncoder().fit(X[col])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        output = X.copy()\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            output[col] = self.encoders[col].transform(X[col])\n",
    "        return output\n",
    "\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        output = X.copy()\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            output[col] = self.encoders[col].inverse_transform(X[col])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify categorical columns\n",
    "cat_cols = ['type', 'other_cat1', 'other_cat2', 'other_cat3', 'other_cat4', 'other_cat5']\n",
    "\n",
    "other = 'sector2'\n",
    "\n",
    "# Force all to string\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].astype(str)\n",
    "    \n",
    "# Instantiate Label Encoder\n",
    "multi = MultiColumnLabelEncoder(columns=cat_cols)\n",
    "\n",
    "# Create new df\n",
    "df_encode = df.copy()\n",
    "\n",
    "# Encode\n",
    "df_encode = multi.fit_transform(df_encode)\n",
    "\n",
    "# Encode just sectors\n",
    "le = LabelEncoder()\n",
    "df_encode.sector2 = df_encode.sector2.astype(str)\n",
    "df_encode.sector2 = le.fit_transform(df_encode.sector2)\n",
    "\n",
    "# Re-establish NA\n",
    "df_encode = df_encode.mask(df.isna())\n",
    "\n",
    "# Imputation on sectors\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_impute = pd.DataFrame(imputer.fit_transform(df_encode), \n",
    "                        columns = df_encode.columns)\n",
    "\n",
    "# Establish levels for sector\n",
    "df_impute.sector2 = [int(x) for x in df_impute.sector2]\n",
    "\n",
    "# Save pickle\n",
    "df_impute.to_pickle('df_impute.pkl')\n",
    "\n",
    "####### Inverse Transform - get labeled data\n",
    "# Recode sector\n",
    "df_impute.sector2 = le.inverse_transform(df_impute.sector2)\n",
    "\n",
    "# Inverse transform cat cols\n",
    "df_impute[cat_cols] = multi.inverse_transform(df_impute[cat_cols].astype(int))\n",
    "\n",
    "# Save pickle\n",
    "df_impute.to_pickle('df_impute_label.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the dataset is quite imbalanced in regard to conversion. To alleviate this, we can use SMOTE oversampling techniques to create synthetic converted data. In using this data, we will also need to set aside a portion of the original set to validate, as SMOTE can often overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    60688\n",
      "1.0      708\n",
      "Name: label, dtype: int64\n",
      "0.0    60688\n",
      "1.0    20027\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Use SMOTE Oversampling\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Load Imputed\n",
    "df_impute = pd.read_pickle('df_impute.pkl')\n",
    "\n",
    "# Specify X, y\n",
    "X = df_impute.drop(columns = 'label')\n",
    "y = df_impute.label\n",
    "\n",
    "# Set aside non oversampled for testing\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y,\n",
    "                                                stratify=y, \n",
    "                                                test_size=0.33)\n",
    "\n",
    "# Create validation set for later\n",
    "df_validate = X_validate\n",
    "df_validate['label']=y_validate\n",
    "df_validate.to_pickle('df_validate.pkl')\n",
    "\n",
    "# Reestablish dataframe to oversample\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "# Specify overall categorical columns\n",
    "full_cat_cols = [x for x in df_impute.columns if x not in ['ordinal1', 'ordinal2', 'label']]\n",
    "\n",
    "# Get categorical column indices\n",
    "cat_col_idx = [X.columns.get_loc(c) for c in full_cat_cols if c in X]\n",
    "\n",
    "# Create over and under\n",
    "sme = SMOTENC(categorical_features=cat_col_idx, sampling_strategy=0.33)\n",
    "X_res, y_res = sme.fit_resample(X, y)\n",
    "\n",
    "# Check counts\n",
    "print(y.value_counts())\n",
    "print(y_res.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create oversampled df\n",
    "df_smote = X_res.copy()\n",
    "df_smote['label'] = y_res\n",
    "\n",
    "# Save to pickle\n",
    "df_smote.to_pickle('df_smote.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
